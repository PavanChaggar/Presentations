{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<script>\n",
    "    function findAncestor (el, name) {\n",
    "        while ((el = el.parentElement) && el.nodeName.toLowerCase() !== name);\n",
    "        return el;\n",
    "    }\n",
    "    function colorAll(el, textColor) {\n",
    "        el.style.color = textColor;\n",
    "        Array.from(el.children).forEach((e) => {colorAll(e, textColor);});\n",
    "    }\n",
    "    function setBackgroundImage(src, textColor) {\n",
    "        var section = findAncestor(document.currentScript, \"section\");\n",
    "        if (section) {\n",
    "            section.setAttribute(\"data-background-image\", src);\n",
    "\t\t\tif (textColor) colorAll(section, textColor);\n",
    "        }\n",
    "    }\n",
    "</script>\n",
    "\n",
    "<script>\n",
    "    setBackgroundImage(\"/Users/pavanchaggar/Documents/ResearchDocs/Presentations/background_imgs/title-slide-background.png\");\n",
    "    \n",
    "</script>\n",
    "\n",
    "<h1 style=\"color:White;\">Variational Bayesian Inference</h1>\n",
    "<h2 style=\"color:White;\" >Pavan Chaggar</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<script>\n",
    "    setBackgroundImage(\"/Users/pavanchaggar/Documents/ResearchDocs/Presentations/background_imgs/ox-math-background.png\");\n",
    "</script>\n",
    "## Outline: \n",
    "\n",
    "* Introduction and Motivation \n",
    "* Variational Bayes\n",
    "* Application to modelling Alzheimer's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<script>\n",
    "    setBackgroundImage(\"/Users/pavanchaggar/Documents/ResearchDocs/Presentations/background_imgs/ox-math-background.png\");\n",
    "</script>\n",
    "\n",
    "## Introduction and Motivation: Why do Bayesian Inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<script>\n",
    "    setBackgroundImage(\"/Users/pavanchaggar/Documents/ResearchDocs/Presentations/background_imgs/ox-math-background.png\");\n",
    "</script>\n",
    "\n",
    "Very simply, we wish to assess the evidence for some hypothesis given some data. Or, alternatively, ask what does this model tell us about the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<script>\n",
    "    setBackgroundImage(\"/Users/pavanchaggar/Documents/ResearchDocs/presentations/ox-math-background.png\");\n",
    "</script>\n",
    "We can do this using the Bayes-Price-Laplace rule, as follows.\n",
    "\n",
    "For observations $\\mathbf{x} = x_{1:n}$ and latent variables  $\\mathbf{z} = z_{1:m}$, we have a join density\n",
    "\n",
    "$$ p(\\mathbf{x}, \\mathbf{z}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To evalulate a particular hypothesis, we need to evaluate the posterior $ p(\\mathbf{z} \\mid \\mathbf{x}) $, thus we decompose the joint distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ p(\\mathbf{x}, \\mathbf{z}) = p(\\mathbf{x} \\mid \\mathbf{z})p(\\mathbf{z}) = p(\\mathbf{z} \\mid \\mathbf{x})p(\\mathbf{x}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "and we can obtain the posterior by the Bayes-Price-Laplace rule: \n",
    "\n",
    "$$p(\\mathbf{z} \\mid \\mathbf{x}) = \\frac{p(\\mathbf{x} \\mid \\mathbf{z})p(\\mathbf{z})}{p(\\mathbf{x})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<script>\n",
    "    setBackgroundImage(\"/Users/pavanchaggar/Documents/ResearchDocs/Presentations/background_imgs/ox-math-background.png\");\n",
    "</script>\n",
    "\n",
    "### Bayes-Price-Laplace Rule\n",
    "\n",
    "$$ \\underbrace{p(\\mathbf{z} \\mid \\mathbf{x})}_{posterior} = \\frac{\\overbrace{p(\\mathbf{x} \\mid \\mathbf{z})}^{likelihood}\\overbrace{p(\\mathbf{z})}^{prior}}{\\underbrace{p(\\mathbf{x})}_{evidence}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- Likelihood: Probability that a particular set of parameter values generate the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Prior: Probability representing our initial beliefs about the parameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Evidence: Normalising factor; probability of observing our data (given our model). Otherwise known as the marginal likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Posterior: Probability that some data are _caused_ by some set of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<script>\n",
    "    setBackgroundImage(\"/Users/pavanchaggar/Documents/ResearchDocs/Presentations/background_imgs/ox-math-background.png\");\n",
    "</script>\n",
    "\n",
    "So, why can't we just calculate this? The problem lies in the denominator, evaluated as: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ {p(\\mathbf{x})} = \\int p(\\mathbf{x} , \\mathbf{z}) d\\mathbf{z} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Typically, this integral is difficult to solve analytically or computationally, primarily since the problem becomes intractable with increasing numbers of latent variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Therefore, we need a way to evaluate the posterior distribution without computing the denominator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To do this, we turn to approximate Bayesian inference. While there many ways to approximate the posterior distribution, we use variational inference for computational efficiency and speed. (More on this later.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<script>\n",
    "    setBackgroundImage(\"/Users/pavanchaggar/Documents/ResearchDocs/Presentations/background_imgs/ox-math-background.png\");\n",
    "</script>\n",
    "\n",
    "## Variational Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<script>\n",
    "    setBackgroundImage(\"/Users/pavanchaggar/Documents/ResearchDocs/Presentations/background_imgs/ox-math-background.png\");\n",
    "</script>\n",
    "\n",
    "VB aims to circumvent the large time complexity by approaching the problem through optimisation. \n",
    "\n",
    "The process begins by positing a contrived _approximate_ density, $\\mathfrak{D}$ of latent variables $\\mathbf{z}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then, within this family, we wish to find the approximate posteriors that minimise the Kullback-Leibler divergence to the true posterior. \n",
    "\n",
    "$$ q^{*}(\\mathbf{z}) = \\underset{q(\\mathbf{z}) \\in \\mathfrak{D}}{argmin} \\mathrm{KL}(q(\\mathbf{z}) \\mid \\mid p(\\mathbf{z} \\mid \\mathbf{x})) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, this still depends on the intracable evidence..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<script>\n",
    "    setBackgroundImage(\"/Users/pavanchaggar/Documents/ResearchDocs/Presentations/background_imgs/ox-math-background.png\");\n",
    "</script>\n",
    "\n",
    "To make this more clear, we can rewrite the expression using Bayes' rule: \n",
    "\n",
    "$$ q^{*}(\\mathbf{z}) = \\underset{q(\\mathbf{z}) \\in \\mathfrak{D}}{argmin} \\mathrm{KL}\\bigg(q(\\mathbf{z}) \\mid \\mid \\frac{p(\\mathbf{x} \\mid \\mathbf{z})p(\\mathbf{z})}{p(\\mathbf{x})}\\bigg) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can avoid evidence term with the following manipulations. First, let's break down the KL divergence, recalling that the KL divergence can otherwise be expressed as the expectated value of the first argument minus the expected value of the second argument: \n",
    "\n",
    "$$ \\mathrm{KL}(q(\\mathbf{z}) \\mid \\mid p(\\mathbf{z} \\mid \\mathbf{x})) = \\mathbb{E}[\\log q(\\mathbf{\\mathbf{z}})] - \\mathbb{E}[\\log p(\\mathbf{x} \\mid \\mathbf{z})] - \\mathbb{E}[\\log p(\\mathbf{z})] + \\log p(\\mathbf{x}) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And simply rearrange and drop the constant evidence term: \n",
    "\n",
    "\\begin{align}\n",
    "    \\label{eqn:free-energy2}\n",
    "-\\mathrm{KL}(q(\\mathbf{z}) \\mid \\mid p(\\mathbf{z} \\mid \\mathbf{x})) &> \\mathbb{E}[\\log p(\\mathbf{x} \\mid \\mathbf{z})] - \\mathbb{E}[\\log q(\\mathbf{z})] + \\mathbb{E}[\\log p(\\mathbf{z})] \\\\\n",
    "    \\label{eqn:free-energy3}\n",
    "    \\mathbf{F} &= \\underbrace{\\mathbb{E}[\\log p(\\mathbf{x} \\mid \\mathbf{z})]}_{accuracy} - \\underbrace{\\mathrm{KL}(q(\\mathbf{z}) \\mid \\mid p(\\mathbf{z}))}_{complexity}\n",
    "\\end{align}  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<script>\n",
    "    setBackgroundImage(\"/Users/pavanchaggar/Documents/ResearchDocs/Presentations/background_imgs/ox-math-background.png\");\n",
    "</script>\n",
    "\n",
    "There are a number of methods with which to optmise the free energy. Here, we focus on analytic variational Bayes, which uses the calculus of variations to perform the optimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Importantly, we must make a mean-field assumption such that: \n",
    "\n",
    "$$ q(z) = \\prod_{i} q_{z_{i}}(z_i) $$\n",
    "\n",
    "\n",
    "After rewriting $\\mathbf{F}$ as a density we formulate an Euler-Lagrange equation that has the following solution:\n",
    "\n",
    "$$ p(z_{i}) \\propto \\int p(x \\mid z)p(z)q(z_{-i})dz_{-i} $$ \n",
    "\n",
    "Where $-i$ represents the latent variables that are not in $i$.\n",
    "\n",
    "\n",
    "For a full and proper derivation of this, see: Beal, M.J. (2003) Variational Algorithms for Approximate Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<script>\n",
    "    setBackgroundImage(\"/Users/pavanchaggar/Documents/ResearchDocs/Presentations/background_imgs/ox-math-background.png\");\n",
    "</script>\n",
    "\n",
    "\n",
    "## Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<script>\n",
    "    setBackgroundImage(\"/Users/pavanchaggar/Documents/ResearchDocs/Presentations/background_imgs/ox-math-background.png\");\n",
    "</script>\n",
    "\n",
    "\n",
    "\n",
    "In the context of Alzheimer's modelling, we can use the variational inference framework to estimate the posterior distributions of our model parameters given some data.\n",
    "\n",
    "The first step in doing this is to specify a generative model (a data generating process). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<script>\n",
    "    setBackgroundImage(\"/Users/pavanchaggar/Documents/ResearchDocs/Presentations/background_imgs/ox-math-background.png\");\n",
    "</script>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Forward Model \n",
    "\n",
    "We can set up a forward model in the following way and begin variational inference. \n",
    "\n",
    "Firstly, we assume the data, $\\mathbf{y}$ are generated by a non-linear forward model with normally distributed noise:\n",
    "\n",
    "$$ \\mathbf{y} = g(\\boldsymbol{\\theta}) + \\epsilon $$ \n",
    "$$ \\epsilon \\approx \\mathcal{N}(0, \\Phi^{-1}) $$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Using this forward model, we can parameterise the log-likelihood with the data conditioned on $\\Theta$, a set of independent parameters:\n",
    "\n",
    "$$ p(\\mathbf{y} \\mid \\mathbf{\\Theta}) = \\frac{N}{2} \\log(\\Phi) - \\frac{1}{2}(\\mathbf{y} - g(\\boldsymbol{\\theta}))^{T} \\Phi (\\mathbf{y} - g(\\boldsymbol{\\theta})) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We define priors with the following distributions on $\\mathbf{\\theta}$ and $\\Phi$: \n",
    "\n",
    "$$ p(\\theta \\mid \\mathbf{y}) = MVN(\\mathbf{m_0}, \\Sigma^{-1}_0) $$ \n",
    "\n",
    "$$ p(\\phi \\mid \\mathbf{y}) = Ga(s_0, c_0) $$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Using these, we can construct the approximate log posterior and thus derive equations. \n",
    "\n",
    "$$ L = \\log p(\\mathbf{y} \\mid \\mathbf{\\Theta}) + \\log p(\\theta) + \\log p(\\phi) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<script>\n",
    "    setBackgroundImage(\"/Users/pavanchaggar/Documents/ResearchDocs/Presentations/background_imgs/ox-math-background.png\");\n",
    "</script>\n",
    "\n",
    "\n",
    "Lastly, as per the mean field assumption, we can factorise the approximate posterior into two groups:\n",
    "\n",
    "$$ q(\\Theta \\mid \\mathbf{y}) = q(\\theta_n)q(\\phi_n) $$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "and choose conjucate distributions for their form:\n",
    "\n",
    "$$ q(\\theta \\mid \\mathbf{y}) \\approx MVN(\\theta; \\mathbf{m}, \\Lambda^{-1}) $$ \n",
    "\n",
    "$$ q(\\phi \\mid \\mathbf{y}) \\approx Ga(\\phi; s, c) $$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Remeber, these are two critical assumptions. First, the mean field assumptions ensures we can update each parameter interatively and independently. Second, the conjugacy allows us to derive the updates analytically!\n",
    "\n",
    "For a full derivation of the update rules (and code), see: https://github.com/PavanChaggar/Bayesian_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<script>\n",
    "    setBackgroundImage(\"/Users/pavanchaggar/Documents/ResearchDocs/Presentations/background_imgs/ox-math-background.png\");\n",
    "</script>\n",
    "\n",
    "\n",
    "## A Software Interlude..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<script>\n",
    "    setBackgroundImage(\"/Users/pavanchaggar/Documents/ResearchDocs/Presentations/background_imgs/ox-math-background.png\");\n",
    "</script>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class NetworkFKPP(Model):\n",
    "    def fkpp(self, p, t, theta):\n",
    "        k, a = theta\n",
    "        du = k * (-self.L() @ p) + (a * p) * (1 - p)\n",
    "        return du\n",
    "\n",
    "    def solve(self, p, theta):\n",
    "        return numerically integrate fkpp\n",
    "\n",
    "    def forward(self, u0): \n",
    "        p = u0[:-2]\n",
    "        theta = u0[-2:]\n",
    "        \n",
    "        u = self.solve(p, theta) \n",
    "        return u "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<script>\n",
    "    setBackgroundImage(\"/Users/pavanchaggar/Documents/ResearchDocs/Presentations/background_imgs/ox-math-background.png\");\n",
    "</script>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "p = np.zeros([83]) + 1e-5\n",
    "mask = [25, 26, 39, 40, 66, 67, 80, 81]\n",
    "p[mask] = 0.1\n",
    "\n",
    "k = 5\n",
    "a = 10\n",
    "\n",
    "m = NetworkFKPP(adjacency_matrix)\n",
    "\n",
    "m.t = np.linspace(0,1,100)\n",
    "\n",
    "u0 = np.append(p, [k, a])\n",
    "\n",
    "sim = m.forward(u0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<script>\n",
    "    setBackgroundImage(\"/Users/pavanchaggar/Documents/ResearchDocs/Presentations/background_imgs/ox-math-background.png\");\n",
    "</script>\n",
    "\n",
    "\n",
    "<img align=\"right\" width=\"500\" height=\"750\" src=\"forward_results.png\">\n",
    "\n",
    "This generates a forward model that produces the curve seen in (a) \n",
    "\n",
    "We can generate synthetic data by adding Gaussian noise, $\\mathcal{N}(0,0.1)$\n",
    "\n",
    "Our goal is to infer the initial parameter values underlying (a) from the data (b)\n",
    "\n",
    "We now use variational Bayes to _invert_ our generative model and estimate the posterior distributions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<script>\n",
    "    setBackgroundImage(\"/Users/pavanchaggar/Documents/ResearchDocs/Presentations/background_imgs/ox-math-background.png\");\n",
    "</script>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# set priors \n",
    "p0 = np.zeros([83])\n",
    "k0 = 0\n",
    "a0 = 0\n",
    "\n",
    "u_0 = np.append(p0, [k0, a0])\n",
    "\n",
    "problem = VBProblem(model=m, data=data, init_means=u_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n=100\n",
    "\n",
    "sol, F = problem.infer(n=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<script>\n",
    "    setBackgroundImage(\"/Users/pavanchaggar/Documents/ResearchDocs/presentations/ox-math-background.png\");\n",
    "</script>\n",
    "\n",
    "<img align=\"right\" width=\"500\" height=\"500\" src=\"inference_results.png\">\n",
    "\n",
    "| Parameter      | True Value   | Inferred Value|\n",
    "| :------------- | :----------: | -----------:  |\n",
    "|  $k$           | 5.           | 2.938         |\n",
    "|  $\\alpha$      | 10           | 9.976       \\||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<script>\n",
    "    setBackgroundImage(\"/Users/pavanchaggar/Documents/ResearchDocs/Presentations/background_imgs/ox-math-background.png\");\n",
    "</script>\n",
    "\n",
    "\n",
    "# Work to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Implement different VB methods for more flexibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Create generative models for fewer data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Test other inference methods such as simulation based inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Write a paper, get a DPhil etc... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<script>\n",
    "    setBackgroundImage(\"/Users/pavanchaggar/Documents/ResearchDocs/Presentations/background_imgs/ox-math-background.png\");\n",
    "</script>\n",
    "\n",
    "## Useful References\n",
    "\n",
    "Beal, M.J. (2003)\n",
    "Variational Algorithms for Approximate Bayesian Inference\n",
    "https://cse.buffalo.edu/faculty/mbeal/thesis/\n",
    "\n",
    "Blei, D. et al., (2017)\n",
    "Variational Inference: A Review for Statisticians \n",
    "https://arxiv.org/pdf/1601.00670.pdf\n",
    "\n",
    "CHappell, M. Groves, A.R. Woolrich M.W.\n",
    "The FMRIB Variational Bayes Tutorial\n",
    "https://vb-tutorial.readthedocs.io/en/latest/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
